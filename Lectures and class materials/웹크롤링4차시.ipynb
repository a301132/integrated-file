{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4차시\n",
    "## 자동 웹크롤링\n",
    "#### Selenium을 활용하여 자동 웹크롤링 코드를 작성할 것이다.\n",
    "#### 오늘 배울 것은 코드을 이해를 하고 다음번에 다른 사이트의 크롤링을 사용할 수 있는 방법들을 얘기하겠다.\n",
    "#### 총 3가지의 코드를 보여줄 것이다.\n",
    "#### 1. 유튜브 영상 댓글 가져오기\n",
    "#### 2. 구글 이미지 가져오기\n",
    "#### 3. 국민 신문고 데이터 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아래의 코드들을 다 이해해라는 말은 하지 않겠습니다. 흐름을 이해하고 어떤 식으로 구성되는지를 아시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "유투브 영상의 댓글 수집하기\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1.유튜브에서 검색할 주제 키워드를 입력하세요(예:올리브영):  올리브영\n",
      "2.위 주제로 댓글을 크롤링할 유튜브 영상은 몇건입니까?: 10\n",
      "3.각 동영상에서 추출할 댓글은 몇건입니까?:  10\n",
      "4.크롤링 결과를 저장할 폴더명만 쓰세요(예:c:\\temp\\): C:\\Users\\a3011\\OneDrive\\Desktop\\유튜브댓글모음\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "요청하신 정보로 데이터를 수집중이니 잠시만 기다려 주세요~~^^\n",
      "\n",
      "\n",
      "검색된 영상 중 10 건 동영상의 댓글을 추출하겠습니다\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 번째 동영상의 정보를 수집합니다.\n",
      "댓글 8개\n",
      "================================================================================\n",
      "1 번째 동영상의 조회수는 322 회 이고 수집할 댓글은 총 8개 입니다\n",
      "1 번째 동영상의 제목은 �집에서 쉽게 케어하자! 목주름 미리 관리하기! [뷰티위키] l 올리브영(Oliveyoung) l 소의튜브 입니다\n",
      "================================================================================\n",
      "\n",
      "\n",
      "1 번째 동영상에서 댓글 수집을 시작합니다\n",
      "댓글의 개수가 많아서 시간이 걸릴 수 있으니 잠시 기다려 주세요~~^^\n",
      "\n",
      "\n",
      "1 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "2 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3 번째 동영상의 정보를 수집합니다.\n",
      "댓글 11개\n",
      "================================================================================\n",
      "3 번째 동영상의 조회수는 92928 회 이고 수집할 댓글은 총 11개 입니다\n",
      "3 번째 동영상의 제목은 피부과 직원 10년차! 소의튜브가 알려주는 민감피부 케어 방법! [뷰티위키] l 올리브영(Oliveyoung) l 소의튜브 입니다\n",
      "================================================================================\n",
      "\n",
      "\n",
      "3 번째 동영상에서 댓글 수집을 시작합니다\n",
      "댓글의 개수가 많아서 시간이 걸릴 수 있으니 잠시 기다려 주세요~~^^\n",
      "\n",
      "\n",
      "1 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "2 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 번째 동영상의 정보를 수집합니다.\n",
      "댓글 351개\n",
      "================================================================================\n",
      "5 번째 동영상의 조회수는 330674 회 이고 수집할 댓글은 총 351개 입니다\n",
      "5 번째 동영상의 제목은 올리브영에서 재구매한 제품들! (겨관리/피부좋아지는 바디,스킨케어/머릿결관리/메이크업정착템) | 민스코 Minsco 입니다\n",
      "================================================================================\n",
      "\n",
      "\n",
      "5 번째 동영상에서 댓글 수집을 시작합니다\n",
      "댓글의 개수가 많아서 시간이 걸릴 수 있으니 잠시 기다려 주세요~~^^\n",
      "\n",
      "\n",
      "1 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "2 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7 번째 동영상의 정보를 수집합니다.\n",
      "댓글 129개\n",
      "================================================================================\n",
      "7 번째 동영상의 조회수는 15062 회 이고 수집할 댓글은 총 129개 입니다\n",
      "7 번째 동영상의 제목은 반갑습니다 행복한 올리브영입니다..☹ (마스크 진상,계산대 진상��‍♀️) 입니다\n",
      "================================================================================\n",
      "\n",
      "\n",
      "7 번째 동영상에서 댓글 수집을 시작합니다\n",
      "댓글의 개수가 많아서 시간이 걸릴 수 있으니 잠시 기다려 주세요~~^^\n",
      "\n",
      "\n",
      "1 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "2 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9 번째 동영상의 정보를 수집합니다.\n",
      "댓글 366개\n",
      "================================================================================\n",
      "9 번째 동영상의 조회수는 172299 회 이고 수집할 댓글은 총 366개 입니다\n",
      "9 번째 동영상의 제목은 ENG) 올리브영 최고등급 회원�의 추천템&신박템&재구매템❗️꿀템만 보여드립니다..❗️0live Young Items Recommendation 입니다\n",
      "================================================================================\n",
      "\n",
      "\n",
      "9 번째 동영상에서 댓글 수집을 시작합니다\n",
      "댓글의 개수가 많아서 시간이 걸릴 수 있으니 잠시 기다려 주세요~~^^\n",
      "\n",
      "\n",
      "1 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "2 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11 번째 동영상의 정보를 수집합니다.\n",
      "댓글 196개\n",
      "================================================================================\n",
      "11 번째 동영상의 조회수는 142417 회 이고 수집할 댓글은 총 196개 입니다\n",
      "11 번째 동영상의 제목은 2층짜리 올리브영 통째로 빌려 쇼핑하기｜보타닉힐보 광고 입니다\n",
      "================================================================================\n",
      "\n",
      "\n",
      "11 번째 동영상에서 댓글 수집을 시작합니다\n",
      "댓글의 개수가 많아서 시간이 걸릴 수 있으니 잠시 기다려 주세요~~^^\n",
      "\n",
      "\n",
      "1 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "2 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "13 번째 동영상의 정보를 수집합니다.\n",
      "댓글 200개\n",
      "================================================================================\n",
      "13 번째 동영상의 조회수는 61470 회 이고 수집할 댓글은 총 200개 입니다\n",
      "13 번째 동영상의 제목은 [올리브영추천템�]1000만원 쓴 올영알바생과 2000만원 쓴 올리브영 점장님의 20년 마지막 올리브영세일 추천템� | 꿀랄라 입니다\n",
      "================================================================================\n",
      "\n",
      "\n",
      "13 번째 동영상에서 댓글 수집을 시작합니다\n",
      "댓글의 개수가 많아서 시간이 걸릴 수 있으니 잠시 기다려 주세요~~^^\n",
      "\n",
      "\n",
      "1 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "2 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "15 번째 동영상의 정보를 수집합니다.\n",
      "댓글 161개\n",
      "================================================================================\n",
      "15 번째 동영상의 조회수는 13544 회 이고 수집할 댓글은 총 161개 입니다\n",
      "15 번째 동영상의 제목은 봄맞이 올리브영 추천템�(봄 향수/섀도우팔레트/블러셔/기초/헤어 추천템) 입니다\n",
      "================================================================================\n",
      "\n",
      "\n",
      "15 번째 동영상에서 댓글 수집을 시작합니다\n",
      "댓글의 개수가 많아서 시간이 걸릴 수 있으니 잠시 기다려 주세요~~^^\n",
      "\n",
      "\n",
      "1 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "2 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "17 번째 동영상의 정보를 수집합니다.\n",
      "댓글 775개\n",
      "================================================================================\n",
      "17 번째 동영상의 조회수는 281931 회 이고 수집할 댓글은 총 775개 입니다\n",
      "17 번째 동영상의 제목은 �올리브영 만원이하~ 만원대� 꿀템 추천 입니다\n",
      "================================================================================\n",
      "\n",
      "\n",
      "17 번째 동영상에서 댓글 수집을 시작합니다\n",
      "댓글의 개수가 많아서 시간이 걸릴 수 있으니 잠시 기다려 주세요~~^^\n",
      "\n",
      "\n",
      "1 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "2 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19 번째 동영상의 정보를 수집합니다.\n",
      "댓글 240개\n",
      "================================================================================\n",
      "19 번째 동영상의 조회수는 38917 회 이고 수집할 댓글은 총 240개 입니다\n",
      "19 번째 동영상의 제목은 ENG) 올리브영 VVIP가 쟁이는 숨은 꿀템 인생템❗️ 올해만 벌써 200만원씀� 수부지인생쿠션 | 입술케어 | 남친뻑가는향수 | 인생립 | 촉촉블러셔 | 최모나choimona 입니다\n",
      "================================================================================\n",
      "\n",
      "\n",
      "19 번째 동영상에서 댓글 수집을 시작합니다\n",
      "댓글의 개수가 많아서 시간이 걸릴 수 있으니 잠시 기다려 주세요~~^^\n",
      "\n",
      "\n",
      "1 번째 페이지의 댓글을 수집하는 중입니다~\n",
      "2 번째 페이지의 댓글을 수집하는 중입니다~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-f2a5e052a2d1>:255: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  youtube_reple['댓글작성자명']=pd.Series(writer2)\n",
      "<ipython-input-2-f2a5e052a2d1>:256: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  youtube_reple['댓글작성일자']=pd.Series(wdate2)\n",
      "<ipython-input-2-f2a5e052a2d1>:257: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  youtube_reple['댓글 내용']=pd.Series(reple3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====================================================================================================\n",
      "1.요청된 총 10 건 동영상 리뷰 중에서 실제 크롤링 된 리뷰수는 1 건입니다\n",
      "2.총 소요시간은 506.2 초 입니다 \n",
      "3.파일 저장 완료: txt 파일명 : C:\\Users\\a3011\\OneDrive\\Desktop\\유튜브댓글모음2021-03-25-09-07-50-올리브영\\2021-03-25-09-07-50-올리브영.txt \n",
      "4.파일 저장 완료: csv 파일명 : C:\\Users\\a3011\\OneDrive\\Desktop\\유튜브댓글모음2021-03-25-09-07-50-올리브영\\2021-03-25-09-07-50-올리브영.csv \n",
      "5.파일 저장 완료: xls 파일명 : C:\\Users\\a3011\\OneDrive\\Desktop\\유튜브댓글모음2021-03-25-09-07-50-올리브영\\2021-03-25-09-07-50-올리브영.xls \n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "'''Step 1. 필요한 모듈과 라이브러리를 로딩합니다.'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import numpy \n",
    "import pandas as pd   \n",
    "import xlwt \n",
    "import random\n",
    "import os\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "'''Step 2. 사용자에게 검색어 키워드를 입력 받습니다.'''\n",
    "\n",
    "print(\"=\" *80)\n",
    "print(\"연습문제 : 유투브 영상의 댓글 수집하기\")\n",
    "print(\"=\" *80)\n",
    "\n",
    "'''input 함수를 사용하여 컴퓨터와 상호작용을 합니다.'''\n",
    "\n",
    "query_txt=input(\"1.유튜브에서 검색할 주제 키워드를 입력하세요(예:올리브영): \")\n",
    "cnt = int(input('2.위 주제로 댓글을 크롤링할 유튜브 영상은 몇건입니까?:'))\n",
    "reple_cnt = int(input('3.각 동영상에서 추출할 댓글은 몇건입니까?: '))\n",
    "f_dir = input(\"4.크롤링 결과를 저장할 폴더명만 쓰세요(예:c:\\\\temp\\\\):\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"요청하신 정보로 데이터를 수집중이니 잠시만 기다려 주세요~~^^\")\n",
    "\n",
    "\n",
    "\n",
    "'''저장될 파일위치와 이름을 지정합니다'''\n",
    "'''현재시간을 받아줍니다.'''\n",
    "\n",
    "now = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "\n",
    "\n",
    "'''저장할 폴더를 만들어줍니다.'''\n",
    "\n",
    "os.makedirs(f_dir+s+'-'+query_txt)\n",
    "os.chdir(f_dir+s+'-'+query_txt)\n",
    "\n",
    "\n",
    "'''저장할 파일명과 경로를 작성해줍니다.'''\n",
    "\n",
    "ff_name=f_dir+s+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.txt'\n",
    "fc_name=f_dir+s+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.csv'\n",
    "fx_name=f_dir+s+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.xls'\n",
    "\n",
    "'''얼마나 걸렸는지 시간을 보기위해 실행시킵니다.'''\n",
    "\n",
    "s_time = time.time( )\n",
    "\n",
    "\n",
    "'''Step 3. 크롬 드라이버를 사용해서 웹 브라우저를 실행합니다.'''\n",
    "\n",
    "path = \"C:\\\\Users\\\\a3011\\\\Downloads\\\\chromedriver_win32\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "driver.get('https://www.youtube.com')\n",
    "\n",
    "'''아래 코드는 5초뒤에 아래코드를 시작한다는 것입니다.'''\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "element = driver.find_element_by_name(\"search_query\")\n",
    "element.send_keys(query_txt)\n",
    "element.submit()\n",
    "time.sleep(2)\n",
    "\n",
    "'''\n",
    "검색 결과가 첫 화면에 20개가 나옵니다.\n",
    "만약 사용자가 입력한 동영상 개수가 20개가 넘어갈 경우 스크롤다운해서 갯수를\n",
    "먼저 만든 후 URL 을 추출하여 URL 을 완성해야 합니다\n",
    "'''\n",
    "\n",
    "'''유튜브 댓글은 스크롤을 내려야 더 추가가 되기 때문에 스크롤 내리는 코드를 작성해줍니다.'''\n",
    "\n",
    "def scroll_down(driver):\n",
    "    #driver.execute_script(\"window.scrollTo(500,document.body.scrollHeight);\")\n",
    "    driver.execute_script(\"window.scrollBy(0,3000);\") # 한페이지 20개씩 출력값\n",
    "    time.sleep(5)\n",
    "\n",
    "'''유튜브 댓글은 한번에 20개를 보여주며, 스크롤을 내릴때마다 20개씩 추가로 보여줍니다.'''\n",
    "'''그래서 댓글을 긁어오는 양에 따라 스크롤을 몇번 내려줄지 정하는 코드입니다.'''\n",
    "\n",
    "if reple_cnt < 20 :\n",
    "        page_cnt = 1 \n",
    "else :\n",
    "        page_cnt = math.ceil(cnt/20)\n",
    "        \n",
    "if cnt > 20 :\n",
    "           \n",
    "    i = 1\n",
    "    while ( i <=  page_cnt) :\n",
    "        print(\"화면을 %s 회 아래로 스크롤 합니다\" %i)\n",
    "        scroll_down(driver)\n",
    "        time.sleep(1)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "'''검색된 유튜브 영상의 URL 을 추출합니다'''\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "html = driver.page_source\n",
    "soup1 = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "count = 0\n",
    "item=[]\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in soup1.find_all('a','yt-simple-endpoint style-scope ytd-video-renderer'):\n",
    "          \n",
    "    item.append(i['href'])\n",
    "    count += 1\n",
    "\n",
    "    if count == cnt :\n",
    "        break\n",
    "\n",
    "print(\"검색된 영상 중 %s 건 동영상의 댓글을 추출하겠습니다\" %cnt)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "'''비트맵 이미지 아이콘을 위한 대체 딕셔너리를 만든다'''\n",
    "bmp_map = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)\n",
    "\n",
    "\n",
    "'''\n",
    "수집된 전체 URL 중에서 사용자가 입력한 수만큼의 동영상의 댓글을 수집합니다.\n",
    "먼저 전체 URL 을 완성 -> 동영상 페이지 열기 -> 댓글 수집 -> 다음 동영상 순으로 진행합니다\n",
    "'''\n",
    "\n",
    "full_url=[]\n",
    "url_cnt = 0\n",
    "for x in range(0,len(item)) :\n",
    "    url_cnt += 1\n",
    "    url = 'https://www.youtube.com/' +item[x]\n",
    "    full_url.append(url)\n",
    "    if url_cnt == cnt :\n",
    "        break\n",
    "\n",
    "\n",
    "play_cnt = 1\n",
    "\n",
    "'''파싱한 데이터들을 저장할 공간을 지정해 줍니다.'''\n",
    "\n",
    "\n",
    "url2=[]\n",
    "reple2=[]\n",
    "reple3=[]\n",
    "reple4=[]\n",
    "writer2=[]\n",
    "wdate2=[]\n",
    "\n",
    "\n",
    "'''엑셀 형태로 저장하기 위해 실행하는 코드입니다.'''\n",
    "wb=openpyxl.Workbook( )\n",
    "wb.save(fx_name)\n",
    "\n",
    "''' 동영상의 기본정보를 추출하는 코드입니다.'''\n",
    "\n",
    "\n",
    "for y in range(0,len(full_url)) :\n",
    "    driver.get(full_url[y])\n",
    "    time.sleep(10)\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    print(\"%s 번째 동영상의 정보를 수집합니다.\" %play_cnt)\n",
    "\n",
    "    i = 1\n",
    "    while ( i <=  page_cnt) :\n",
    "    #print(\"화면을 %s 회 아래로 스크롤 합니다\" %i)\n",
    "        scroll_down(driver)\n",
    "        time.sleep(10)\n",
    "        i += 1\n",
    "      \n",
    "    html = driver.page_source\n",
    "    soup2 = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "    t_count_0 = soup2.select('#count')\n",
    "    t_count_1 = t_count_0[1].get_text()\n",
    "    t_count_2 = t_count_1.replace(\",\",\"\")\n",
    "    t_count_3 = re.search(\"\\d+\",t_count_2)\n",
    "    t_count_4 = int(t_count_3.group())\n",
    "\n",
    "    t_title_1 = soup2.select('#info-contents')\n",
    "    t_title_2 = t_title_1[0].find('h1').get_text()\n",
    "    t_title_3 = t_title_2.translate(bmp_map).replace(\"\\n\",\"\")\n",
    "\n",
    "    t_view_1 = soup2.find('yt-formatted-string','count-text style-scope ytd-comments-header-renderer').get_text()\n",
    "    t_view_2 = t_view_1.replace(\",\" ,\"\")\n",
    "    t_view_3 = re.search(\"\\d+\",t_view_2)\n",
    "    print(t_view_1)\n",
    "    t_view_4 = int(t_view_3.group())\n",
    "\n",
    "    print(\"=\" *80)\n",
    "    print(\"%s 번째 동영상의 조회수는 %s 회 이고 수집할 댓글은 총 %s개 입니다\" %(play_cnt,t_count_4,t_view_4))  \n",
    "    print(\"%s 번째 동영상의 제목은 %s 입니다\" %(play_cnt,t_title_3))\n",
    "    print(\"=\" *80)\n",
    "    print(\"\\n\")\n",
    "    print(\"%s 번째 동영상에서 댓글 수집을 시작합니다\" %play_cnt)\n",
    "    print(\"댓글의 개수가 많아서 시간이 걸릴 수 있으니 잠시 기다려 주세요~~^^\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "'''Step 5. 화면을 스크롤해서 아래로 이동합니다'''\n",
    "    i = 1\n",
    "    while ( i <=  page_cnt+1) :\n",
    "        print(\"%s 번째 페이지의 댓글을 수집하는 중입니다~\" %i)\n",
    "        scroll_down(driver)\n",
    "        time.sleep(0.5)\n",
    "        i += 1\n",
    "\n",
    "'''Step 6. 내용을 수집합니다'''\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup3 = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "    count = 0\n",
    "    d2 = 0\n",
    "\n",
    "    reple_result = soup3.select('#comments > #sections > #contents')\n",
    "\n",
    "    for a in range(0,reple_cnt+1) :\n",
    "        count += 1\n",
    "              \n",
    "        f = open(ff_name, 'a',encoding='UTF-8')\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"------------------------------------------------------------------\"+\"\\n\")\n",
    "\n",
    "             # 댓글 작성자\n",
    "        try : \n",
    "            writer = reple_result[0].select(\"#author-text > span\")[a].get_text().replace(\"\\n\",\"\").strip()\n",
    "        except IndexError :\n",
    "            print('error')\n",
    "            break\n",
    "        else :\n",
    "            print(\"\\n\")\n",
    "            print(\"%s 번째 영상의 %s 번째  댓글 \" %(play_cnt,count))\n",
    "            print(\"-\" *70)\n",
    "\n",
    "                # 유튜브 URL 주소\n",
    "            print(\"1.URL 주소:\",full_url[y])\n",
    "            f.write(\"1.유투브 URL주소: \" + full_url[y] + \"\\n\")\n",
    "            url2.append(full_url[y])\n",
    "                \n",
    "            print(\"2.댓글 작성자명:\", writer)\n",
    "            f.write(\"2.댓글 작성자명: \" + writer + \"\\n\")\n",
    "            writer2.append(writer)\n",
    "              \n",
    "            # 댓글 작성일자\n",
    "            wdate = reple_result[0].select(\"yt-formatted-string.published-time-text > a.yt-simple-endpoint\")[a].get_text().replace(\"\\n\",\"\")\n",
    "            \n",
    "            print(\"3.댓글 작성일자:\",wdate)\n",
    "            f.write(\"3.댓글 작성일자: \" + wdate + \"\\n\")\n",
    "            wdate2.append(wdate)\n",
    "              \n",
    "            # 댓글 내용\n",
    "            reple1 = reple_result[0].select('#expander > #content > #content-text')[a].get_text().replace(\"\\n\",\"\")\n",
    "            \n",
    "            reple2=reple1.translate(bmp_map).replace(\"\\n\",\"\")\n",
    "            print(\"4.댓글 내용:\",reple2)\n",
    "            f.write(\"4.댓글 내용: \" + reple2 + \"\\n\")\n",
    "            reple3.append(reple2)\n",
    "                \n",
    "              \n",
    "            f.close( )\n",
    "              \n",
    "            if count == reple_cnt :\n",
    "                break\n",
    "              \n",
    "    time.sleep(2)       \n",
    "    play_cnt += 1                \n",
    "\n",
    "'''step 9. 엑셀 형태로 저장하기'''\n",
    "    \n",
    "              \n",
    "youtube_reple = pd.DataFrame()\n",
    "youtube_reple['URL 주소']=url2\n",
    "youtube_reple['댓글작성자명']=pd.Series(writer2)\n",
    "youtube_reple['댓글작성일자']=pd.Series(wdate2)\n",
    "youtube_reple['댓글 내용']=pd.Series(reple3)\n",
    "\n",
    "\n",
    "# csv 형태로 저장하기\n",
    "youtube_reple.to_csv(fc_name,encoding=\"utf-8-sig\",index=True)\n",
    "\n",
    "# 엑셀 형태로 저장하기\n",
    "youtube_reple.to_excel(fx_name ,index=True)\n",
    "\n",
    "e_time = time.time( )\n",
    "t_time = e_time - s_time\n",
    "\n",
    "# txt 파일에 크롤링 요약 정보 저장하기\n",
    "orig_stdout = sys.stdout\n",
    "f = open(ff_name, 'a',encoding='UTF-8')\n",
    "sys.stdout = f\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"=\" *50)\n",
    "print(\"총 소요시간은 %s 초 이며,\" %t_time)\n",
    "print(\"총 저장 건수는 %s 건 입니다 \" %count)\n",
    "print(\"=\" *50)\n",
    "\n",
    "sys.stdout = orig_stdout\n",
    "f.close( )\n",
    "\n",
    "\n",
    "print(\"\\n\") \n",
    "print(\"=\" *100)\n",
    "print(\"1.요청된 총 %s 건 동영상 리뷰 중에서 실제 크롤링 된 리뷰수는 %s 건입니다\" %(cnt,count))\n",
    "print(\"2.총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"3.파일 저장 완료: txt 파일명 : %s \" %ff_name)\n",
    "print(\"4.파일 저장 완료: csv 파일명 : %s \" %fc_name)\n",
    "print(\"5.파일 저장 완료: xls 파일명 : %s \" %fx_name)\n",
    "print(\"=\" *100)\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "driver.close( )\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''구글 이미지 수집용 크롤러'''\n",
    "\n",
    "'''Step 1. 필요한 모듈과 라이브러리를 로딩합니다.'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import urllib.request\n",
    "import urllib\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "'''Step 2. 필요한 정보를 입력 받습니다.'''\n",
    "\n",
    "\n",
    "print(\"=\" *80)\n",
    "print(\"구글 사이트에서 이미지를 검색하여 수집하는 크롤러 입니다 \")\n",
    "print(\"=\" *80)\n",
    "\n",
    "query_txt = input('1.크롤링할 이미지의 키워드는 무엇입니까?: ')\n",
    "cnt = int(input('2.크롤링 할 건수는 몇건입니까?: '))\n",
    "\n",
    "real_cnt = math.ceil(cnt / 50) # 실제 크롤링 할 페이지 수\n",
    "\n",
    "f_dir=input('3.파일이 저장될 경로만 쓰세요(예: c:\\\\temp\\\\ ) : ')\n",
    "\n",
    "if f_dir =='' :\n",
    "    f_dir = \"c:\\\\temp\\\\\"\n",
    "print(\"\\n\")\n",
    "print(\"요청하신 데이터를 수집 중이오니 잠시만 기다려 주세요~~^^\")\n",
    "\n",
    "#Step 3. 파일을 저장할 폴더를 생성합니다\n",
    "now = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "\n",
    "os.chdir(f_dir)\n",
    "os.makedirs(f_dir+s+'-'+query_txt)\n",
    "os.chdir(f_dir+s+'-'+query_txt)\n",
    "f_result_dir = f_dir+s+'-'+query_txt\n",
    "                \n",
    "\n",
    "'''Step 4. 크롬 드라이버를 사용해서 웹 브라우저를 실행한 후 검색합니다'''\n",
    "\n",
    "\n",
    "s_time = time.time( )\n",
    "\n",
    "path = \"C:\\\\Users\\\\a3011\\\\Downloads\\\\chromedriver_win32\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "driver.get('https://www.google.com')\n",
    "time.sleep(random.randrange(2,5))\n",
    "\n",
    "element = driver.find_element_by_name(\"q\")\n",
    "\n",
    "element.send_keys(query_txt)\n",
    "element.submit()\n",
    "\n",
    "'''Step 5. 아래의 이미지 링크를 선택합니다'''\n",
    "\n",
    "driver.find_element_by_link_text(\"이미지\").click()\n",
    "        \n",
    "'''스크롤 다운 함수 만들기'''\n",
    "\n",
    "def scroll_down(driver):\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "\n",
    "i = 1\n",
    "while (i <= real_cnt):\n",
    "    scroll_down(driver) \n",
    "    i += 1\n",
    "\n",
    "    if i ==  6 :\n",
    "        driver.find_element_by_xpath(\"\"\"//*[@id=\"_sau_imageTab\"]/div[2]/div[8]/a\"\"\").click()\n",
    "\n",
    "'''Step 6. 이미지 추출하여 저장하기 '''\n",
    "\n",
    "file_no = 0\n",
    "count = 1\n",
    "img_src2=[]\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "imgs = driver.find_elements_by_tag_name('img')\n",
    "\n",
    "for img in imgs:\n",
    "        img_src1=img.get_attribute('src')\n",
    "        img_src2.append(img_src1)\n",
    "        count += 1\n",
    "\n",
    "for i in range(2,len(img_src2)+1) :\n",
    "\n",
    "        try :\n",
    "                urllib.request.urlretrieve(img_src2[i],str(file_no)+'.jpg')\n",
    "        except TypeError:\n",
    "                continue\n",
    "        \n",
    "        file_no += 1\n",
    "                \n",
    "        time.sleep(1)\n",
    "        print(\"\\n\")\n",
    "        print(\"%s 번째 이미지 저장중입니다=======\" %file_no)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        if file_no == cnt :\n",
    "                break\n",
    "\n",
    "'''Step 7. 요약 정보를 출력합니다'''\n",
    "e_time = time.time( )\n",
    "t_time = e_time - s_time\n",
    "\n",
    "store_cnt = file_no -1\n",
    "\n",
    "print(\"=\" *70)\n",
    "print(\"총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"총 저장 건수는 %s 건 입니다 \" %file_no)\n",
    "print(\"파일 저장 경로: %s 입니다\" %f_result_dir)\n",
    "print(\"=\" *70)\n",
    "\n",
    "driver.close( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''청와대 국민 신문고 추천순 데이터 수집하기'''\n",
    "\n",
    "'''Step 1. 필요한 모듈과 라이브러리를 로딩합니다.'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import numpy  \n",
    "import pandas as pd  \n",
    "import xlwt\n",
    "\n",
    "import random\n",
    "import os   \n",
    "\n",
    "'''Step 2. 사용자에게 검색어 키워드를 입력 받습니다.'''\n",
    "print(\"=\" *80)\n",
    "print(\" 연습문제: 청와대 국민 신문고 게시판 크롤링하기\")\n",
    "print(\"=\" *80)\n",
    "\n",
    "query_txt = '청와대국민신문고'\n",
    "\n",
    "cnt = int(input(\"   1.크롤링 할 건수는 몇건입니까?: \"))\n",
    "f_dir = input(\"   2.결과 파일을 저장할 폴더명만 쓰세요(예:c:\\\\temp\\\\):\")\n",
    "\n",
    "'''저장될 파일위치와 이름을 지정합니다'''\n",
    "now = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "\n",
    "os.makedirs(f_dir+s+'-'+query_txt)\n",
    "os.chdir(f_dir+s+'-'+query_txt)\n",
    "\n",
    "ff_name=f_dir+s+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.txt'\n",
    "fc_name=f_dir+s+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.csv'\n",
    "fx_name=f_dir+s+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.xls'\n",
    "\n",
    "\n",
    "'''Step 3. 크롬 드라이버를 사용해서 웹 브라우저를 실행합니다.'''\n",
    "s_time = time.time( )\n",
    "\n",
    "path = \"C:\\\\Users\\\\a3011\\\\Downloads\\\\chromedriver_win32\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "driver.get('https://www1.president.go.kr/petitions/best')\n",
    "\n",
    "time.sleep(random.randrange(2,5))  # 2 - 5 초 사이에 랜덤으로 시간 선택\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "page_cnt = math.ceil(cnt / 15)\n",
    "print(\"크롤링 할 총 페이지 번호: \",page_cnt)\n",
    "print(\"=\" *80)\n",
    "\n",
    "'''Step 4.  게시글 요약 정보 출력'''\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "click_cnt = 2   # 클릭한 페이지 번호 지정\n",
    "no = 1\n",
    "cno2=[]\n",
    "title2=[]\n",
    "part2=[]\n",
    "rdate2=[]\n",
    "status2=[]\n",
    "view2=[]\n",
    "\n",
    "for x in range(1, page_cnt+1) :\n",
    "        \n",
    "        print(\"%s 페이지의 요약 내용 수집 시작합니다 =======================\" %x)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        result = soup.select('#cont_view > div.cs_area > div > div > div.board.text')\n",
    "        \n",
    "        for i in range(0, len(result[0].find_all('li')) ):  \n",
    "                \n",
    "            print(\"\\n\")\n",
    "            print(\"%s 번째 국민 신문고 게시글 정보입니다==========================\" %no)\n",
    "            \n",
    "            f = open(ff_name, 'a',encoding='UTF-8')\n",
    "            \n",
    "            # 청원 등록 정보 수집\n",
    "            content = result[0].find_all('li')\n",
    "            \n",
    "            cno = content[i].find(class_='bl_no').text.split(' ')[-1]   \n",
    "            print('1.청원글번호: ',cno ,'\\n')\n",
    "            f.write('\\n')\n",
    "            f.write(\"%s 번째 국민 신문고 게시글 정보입니다==========================\" %no + '\\n')\n",
    "            f.write('1.청원번호:' + cno + '\\n')\n",
    "            cno2.append(cno)\n",
    "            \n",
    "            # 분류 추출\n",
    "            title = content[i].find(class_='bl_category cs').text.split(' ')[-1]\n",
    "            print('2.분류: ' , title , '\\n')\n",
    "            f.write('2.분류:' + title + '\\n')\n",
    "            title2.append(title)\n",
    "            \n",
    "            # 청원만료일 추출\n",
    "            rdate = content[i].find(class_='bl_date light').text\n",
    "            print('3.청원만료일: ' , rdate , '\\n')\n",
    "            f.write('3.청원만료일:' + rdate + '\\n')\n",
    "            rdate2.append(rdate)            \n",
    "            \n",
    "            # 제목 추출\n",
    "            title = content[i].find(class_='cb').text[3:]\n",
    "            print('4.제목: ' , title , '\\n')\n",
    "            f.write('4.제목:' + title + '\\n')\n",
    "            title2.append(title)\n",
    "            \n",
    "            # 참여인원 추출\n",
    "            view = content[i].find(class_='bl_agree cb wv_agree').text.split()[-1]\n",
    "            print('5.참여인원: ' , view , '\\n')\n",
    "            f.write('5.참여인원:' + view + '\\n')\n",
    "            view2.append(view)\n",
    "            \n",
    "            f.close( )\n",
    "            \n",
    "            no += 1\n",
    "            \n",
    "            if no > cnt :\n",
    "                break\n",
    "                \n",
    "        if click_cnt > page_cnt :\n",
    "                break\n",
    "        else :\n",
    "                driver.find_element_by_link_text(\"\"\"%s\"\"\" %click_cnt).click() # 다음 페이지번호 클릭\n",
    "                click_cnt += 1\n",
    "            \n",
    "                time.sleep(2)\n",
    "'''\n",
    "Step 5. 출력 결과를 저장합니다\n",
    "출력 결과를 표(데이터 프레임) 형태로 만들기\n",
    "'''\n",
    "sin = pd.DataFrame()\n",
    "\n",
    "sin['글번호'] = cno2\n",
    "sin['글제목'] = pd.Series(title2) \n",
    "sin['청원만료일'] = pd.Series(rdate2)\n",
    "sin['참여인원'] = pd.Series(view2)\n",
    "            \n",
    "'''csv 형태로 저장하기'''\n",
    "sin.to_csv(fc_name, encoding=\"utf-8-sig\" , index=False)\n",
    "\n",
    "'''엑셀 형태로 저장하기'''\n",
    "sin.to_excel(fx_name,index=False)\n",
    "\n",
    "'''Step 6. 요약 정보 출력하기'''\n",
    "e_time = time.time( )     \n",
    "t_time = e_time - s_time\n",
    "print(\"\\n\") \n",
    "print(\"=\" *100)\n",
    "print(\"총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"파일 저장 완료: txt 파일명 : %s \" %ff_name)\n",
    "print(\"파일 저장 완료: csv 파일명 : %s \" %fc_name)\n",
    "print(\"파일 저장 완료: xls 파일명 : %s \" %fx_name)\n",
    "print(\"=\" *100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 총 3가지의 웹페이지를 파싱했습니다.\n",
    "#### 논문이나 리포터를 작성할 때, 이렇게 데이터를 모아서 사용하시면 됩니다.\n",
    "#### 다음 차시에는 모은 데이터를 어떻게 분석 또는 시각화를 활용하여 의미있는 데이터를 만드는 과정을 합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
